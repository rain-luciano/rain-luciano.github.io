[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Experience\n\nData Visualization Engineer\n\nLENA - Lately, Everything Needs Analytics | September 2023 - Present\n\nI specialize in designing user-friendly dashboards for LENA clients, adhering to industry-standard practices. I have also introduced clients to the Microsoft Fabric environment, implementing Power BI Service tasks like Dataflows, Row Level Security, and data refresh functionalities to aid them in establishing a structure akin to a warehouse.\nI also focus on optimizing and elevating Power BI dashboards to enhance their functionality to a more advanced level, making use of tools like Performance Analyzer and Tabular Editor.\n\n\n\n\nDigital Marketing Analyst\n\nHome Credit Philippines | October 2022 - July 2023\n\nI developed and implemented the ETL processes to consolidate data from diverse sources including Appsflyer, Google Analytics, Meta Business Suite, Tiktok Ads, and Google Ads. This culminated in the creation of a comprehensive Power BI dashboard for the digital marketing team which enabled real-time monitoring and actionable insights for informed decision-making.\nI also developed and maintained SQL queries to construct an audience pool tailored for advertising purposes.\n\n\n\n\n\nEducation\n\nMasters in Applied Mathematics Major in Mathematical Finance | 2022\n\nI worked with the Market & Liquidity Risk Quantitative Analysis team of Metropolitan Bank & Trust Company to forecast BVAL interest rates using traditional statistical models and machine learning models.\n\nStatistical models include: Autoregressive Integrated Moving Average Model, Vector Autoregression, Generalized Autoregressive Conditional Heteroskedasticity model, and the Nelson Siegel Model\nMachine learning models include: k-Nearest Neighbors, Neural Networks, and Random Forest\n\n\n\n\nBachelor of Science in Applied Mathematics with Specialization in Mathematical Finance | 2021\n\n\n\nSkills\n\nSoftware and Platforms\n\nR\nSQL\nPower BI desktop and service\nMicrosoft Excel\nMeta Business Suite\nTikTok Ads\nGoogle Analytics, Google Ads\nAppsflyer\nLaTeX"
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "",
    "text": "According to Cole Nussbaumer Knaflic in her book “Storytelling with Data”, an individual only has around 3-8 seconds to decide whether to continue to look at what you’ve put in front of them or direct their attention to something else.\nI believe this logic can be applied to dashboards. Having a slow dashboard can essentially make it useless. When the end-user decides to open it and it takes at least 8 seconds to load, or when the user changes the slicer and it takes forever to show the results, then they most likely will redirect their attention to doing another task, or worse, analyze in Excel.\nWhen this happens, throwing away this original dashboard is not the only solution (though I have been tempted to do this several times, so I feel you). An already existing dashboard can be elevated, and even made faster. However, in order to do this, we first need to figure out why the dashboard is slow."
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#the-data-model-side",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#the-data-model-side",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "The Data Model Side",
    "text": "The Data Model Side\n\nCheck your schema. Power BI is more optimized for a star schema since the dimensions are helpful for filtering and grouping, while the fact table is for summarizing, all in all improving aggregation. Having a flat file is still acceptable but only for small data sets. If you are dealing with large volumes of data (ex. each row is a transaction, and there can be millions of transactions in a day) then a flat file just won’t cut it. Its performance and refresh is a lot slower, and maintenance will be more difficult down the line.\nDo you have many-to-many relationships? Having this kind of relationship means that the two data sets you are connecting have duplicate values. The problem with this is it introduces ambiguity and may produce incorrect results (ex. in choosing an option from the slicer, from which table will the dashboard show the value?). This can also lead to excessive computing resources since the program will compute against multiple values across multiple tables. Power BI prefers one-to-many relationships.\nLook at your Power Query transformations. Some particular heavy transformations are merge or append. Doing this is not a problem in it of itself, especially if your data set is not at all huge. However, doing merges and appends several times with large data volumes can definitely contribute to slow performance.\nAre your DAX queries complicated? A heavily nested SWITCH statements, for example, will definitely slow down your dashboard as a measure has to re-calculate every time you interact with a report.\nSimilarly, calculated tables and columns are also not ideal, as these have to be re-calculated every time you refresh.\nOverall just making your data model large by retaining unnecessary tables and columns can cause a slow dashboard."
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#the-visual-side",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#the-visual-side",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "The Visual Side",
    "text": "The Visual Side\n\nHow many visuals do you actually need? For example, you have 3 card visuals with the same complex DAX measure filtered to different categories. The DAX measure and the visuals attached to it will render thrice. If the user decides to change the date slicer, then it will re-render again.\nOther visual elements like text boxes and shapes have a negligible impact on the report. However, having lots of them could slow down page refresh since it’s also rendering other things at once–i.e. your DAX queries and main data visuals.\n\nFor the next bullet points, I find that these do not affect dashboard performance necessarily. As a developer, however, these can get frustrating in terms of report navigation and maintenance:\n\nCustom visuals can get tricky. I find that sometimes, the visuals won’t render properly if the DAX measure is not in decimal point for example.\nHaving a large volume of pages can impact PBIX file size and can impede your development process. How do you navigate and maintain 100+ pages? (Not-so) Fun fact: when I add a new page, it goes towards the end. So if I want to put this new page at the beginning, I have to keep dragging this page until it reaches where I want it to be, which is really time-consuming.\nHaving several bookmarks is fine, but only if you have a defined naming convention for them. Imagine maintaining bookmarks of a report with 100+ pages!\n\nI would also like to point out that while optimizing a dashboard with complex DAX measures, a sub-optimal data model with 90 million rows, 100+ pages, and several bookmarks, I was ultimately unable to open my file. This makes optimization impossible, rendering the hard work of the previous developers to be useless."
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#upstream-your-data-model",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#upstream-your-data-model",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "Upstream Your Data Model",
    "text": "Upstream Your Data Model\nFor the data model side, here’s a principle that you should always take to heart:\n\n\n\n\n\n\nRoche’s Maxim of Data Transformation\n\n\n\nData should be transformed as far upstream as possible, and as far downstream as necessary.\n\n\nWhat this means is data transformation should be as close as possible to the data source, but if some transformations are not totally possible to do in the data source itself, then take it downstream (ex. organizational reasons like not having permission to edit the data source).\nLet’s say your data source is Snowflake. If you are collaborating with a data engineer, then you can provide them with the transformations you need and they will do it for you in Snowflake. That way, the data is completely transformed upon ingesting and Power BI will not have to do any additional steps, alleviating the performance of the dashboard.\nAnother example is choosing between Power Query and DAX. If you have the option to choose between the two, the definitely choose Power Query. So, when building the report, the calculation is already applied in the data and therefore does not have to re-render every refresh.\nWith this maxim, here are a couple things you can do:\n\nTry to do all your merges and appends in the data source itself.\nUpstream calculated columns and tables to the data source if possible; if not, try doing it in Power Query.\nTry to see if you can upstream complex and nested DAX measures. For example–if you have to distinct count an ID where the Product = A, then see if you can ingest a pre-filtered table where the Product is already A to reduce computing resources.\nAlready remove unnecessary columns when ingesting the tables into Power BI.\nLastly, keeping in mind the maxim, redesign your data model. Before ingesting the tables into Power BI, have the star schema designed already with the tables you need. Make sure your dimension and fact tables are clean."
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#perform-tests-and-research-on-tools",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#perform-tests-and-research-on-tools",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "Perform Tests and Research on Tools",
    "text": "Perform Tests and Research on Tools\nAfter you are done putting your calculations upstream, consider comparing this optimized version of the file to the un-optimized/original one. See if it makes a difference on performance. If the difference is negligible, then upstreaming the calculations might not be the only solution; you may have to consider the visual side like removing redundant visuals or combining them into one. Perhaps it’s the DAX measures and you should consider simplifying them.\nPerforming tests can give several insights as to what to do to elevate your dashboard. In order to do this, you may use tools such as Power BI’s native Performance Analyzer, or external tools like DAX Studio and Tabular Editor.\nPersonally, I like to use the Advanced Power BI Performance Analyzer by SmartPowerBI. If you’re like me who has a difficult time digesting the numbers in Performance Analyzer, this tool takes the exported JSON file from Performance Analyzer and presents it visually. I discovered this tool from Guy In A Cube’s video and I suggest you watch it too.\nThis is how my teammates and I performed the tests using this tool.\n\nDefine the steps you want to do for the page you want to test on. This way, you can see which step takes the longest and identify a possible bottleneck. It can look like this:\n\nOpen the page\nChoose Product = A\nChoose Start Date from 01/01/2024 to 02/01/2024\n\nPerform the test on the original PBIX and export the JSON file from performing the steps in Performance Analyzer. Do this on the optimized file as well. Use the Advanced Power BI Performance Analyzer to analyze the JSON files.\n\n\n\n\n\n\n\nNote\n\n\n\nResults may vary per test. On some tests, the optimized file can be slower compared to the optimized file. This can also vary per device. Consider doing the tests multiple times on multiple devices then averaging the results.\n\n\n\nCompile and compare the results. Here’s an example of compiled run times. The “Total” row represents the sum of all steps for Run 1 and 2 (so 10.5 + 5.6 + 4.1 = 20.2), while the “Average” column represents the average of the two runs (so the mean of 10.5 and 7.3 is 8.9, and 20.2 and 15.8 is equal to 18). As you can see, we have improved the dashboard on average by 32%. The opening times did not improve as much, though the filtering improved by around 60%. This tells me that our optimization efforts in improving the DAX for these steps worked."
  },
  {
    "objectID": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#revisit-your-dashboard-purposeuse-cases",
    "href": "posts/2024-06-14-Blog1/2024-06-14-Blog1.html#revisit-your-dashboard-purposeuse-cases",
    "title": "A Few Ways to Fix Your Slow Power BI Dashboard",
    "section": "Revisit Your Dashboard Purpose/Use Cases",
    "text": "Revisit Your Dashboard Purpose/Use Cases\nIt’s easy to get carried away and over-engineer your dashboard, especially if the user wants so many features. Like I mentioned in the previous section, my teammates and I had to optimize a 100+ page dashboard with countless of measures and tables.\nAs much as we want to satisfy the requests of our dashboard users, I suggest you challenge them. A page’s function can easily overlap with another, and the same goes for visuals.\nQuestion the dashboard–what does this page tell me? What is the story of this page? Why was this visual chosen? See if you can trim the fat and combine two visuals that tell the same story into one. Better yet, see if you can, for example, separate a 100 page dashboard into three dashboards with different functions and viewers instead.\nKeep this in mind: who are you telling the story to, and what story do they want to see?\nI always base my development/optimization process on these types of dashboards, though each individual’s definitions for these are pretty flexible:\n\nStrategic dashboards display high-level aggregated data. These are not interactive since the goal is to give a “snapshot” of performance. These are viewed by senior-level management so they can make long-term decisions on company strategies. These usually contain success indicator visuals like gauge charts with a “green = good, red = bad” conditional formatting and a high-level time series.\nAnalytical dashboards, compared to strategic dashboards, are highly interactive with high volumes of data. These are for making comparisons, analyzing trends, and overall “investigating” so the viewers can uncover key insights that they might not see at first glance.\nOperational dashboards are for monitoring short-term trends and operation. This is to see a company’s past and present performance, and predict the near future. An example is observing if a marketing campaign is reaching their target for the current month."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rain Luciano",
    "section": "",
    "text": "About Me\nHey, thanks for checking my website out!\nI am a Data Visualization Engineer with a specialization on Power BI. Using my proficiency in dashboard UI/UX design and Power BI best practices, I find fulfillment in bridging the gap between non-technical individuals and data.\nLet’s collaborate on transforming data into actionable insights and compelling stories.\n\n\nBackground\nI graduated from Ateneo de Manila University with a Bachelor’s (2021) and Master’s (2022) degree in Applied Mathematics Major in Mathematical Finance. I am currently a Data Visualization Engineer at LENA - Lately, Everything Needs Analytics."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "A Few Ways to Fix Your Slow Power BI Dashboard\n\n\n\n\n\n\nPower BI\n\n\n\nIf you’ve ever wanted to rage delete Power BI, then here are some learnings from an optimization project my team did that might help you.\n\n\n\n\n\nJun 15, 2024\n\n\nRain Luciano\n\n\n\n\n\n\nNo matching items"
  }
]